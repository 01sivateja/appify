---
title: "Credit_linear_regression"
author: "authour"
date: "2023-11-30"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The objective of this exercise is to examine a dataset made up of data from 400 credit card users in order to identify the elements that affect each user's credit card balance and to forecast each user's average balance using linear regression machine learning. A credit card corporation may carry out such an activity as part of their consumer analysis program. The analysis's findings may reveal whether clients are at risk of a credit default or what kind of conduct to anticipate from potential clients. Furthermore, integrating credit balance data with other relevant information, including credit limit, may help determine a card's credit use, which is a factor in determining a cardholder's credit rating.

Dataset link
 https://www.kaggle.com/code/suzanaiacob/predicting-credit-card-balance-using-regression/input?select=Credit.csv
 

The present exercise will study the Credit Card Balance Data. This is a data frame with 400 observations on the following variables:
•	ID - Identification
•	Income - Income in $10,0000
•	Limit - Credit limit
•	Rating - Credit rating
•	Age - Age in years
•	Education - number of years of education
•	Gender - Male or Female
•	Student - Yes or No
•	Married - Yes or No
•	Ethnicity - African American, Asian or Caucasian
•	Balance - Average credit card balance in 

•	This dataset contains information on features that help build linear model to predict the balance. Machine learning algorithms can be used to create prediction models with this data.   The dataset will be utilized for visualization, exploration, and data cleaning.
	 

## The research question
What features contribute the most when building regression model to predict the balance of a customer. The factors or parameters from the dataset that can be utilized are income, limit, rating, cards and age
Objective:
•	Understand the Dataset 
•	Build classification models to predict  the balance
	
	
## Data-driven, computational approach may be useful
 By employing a computational method, the analysis is grounded in factual facts as opposed to conjecture or subjective views. This reduces biases and offers a more precise and impartial picture of the variables influencing each user's credit card balance. It is possible to analyze a sizable dataset with lots of variables and observations by using computer approaches. Such huge data sets would need a lot of effort and error-prone manual analysis. The analysis may be scaled up to effectively manage massive amounts of data when using a data-driven strategy. The analyzing process can be automated with a computational technique. Without requiring human assistance, the first model may be applied to fresh datasets or data points after it has been trained. This makes it possible for credit card companies to regularly assess and track client. The association between the independent factors (income, rating, cards, age, education, gender) and the dependent variable (credit card balance) may be automatically identified and quantified using machine learning algorithms, such as linear regression in this instance. As a result, the model may learn from the data and create new classifications or predictions depending on observations.


```{r  libraries }
library(ggplot2)
library(tidyverse)
  
data <- read.csv("C:/Users/samso/Downloads/Credit.csv")
head(data)
```
# Load required libraries
dimension
```{r  dim }
dim(data)
```


```{r  null }
#checking for missing values
sum(is.na(data))#
```

```{r   summary }
summary(data)

```

```{r  cols }
colnames(data)

```


```{r outliers, echo=FALSE}
#some outliers present
boxplot(data$Income)
boxplot(Balance ~ Student, data = data)

```

```{r hist, echo=FALSE}
hist(data$Balance)
```



```{r  cor }
#CORRELATION analysis
tmp <- data %>% 
  dplyr::select( "Income",  "Limit" ,    "Rating",  "Cards"   ,  "Age"  ,  "Balance")
head(tmp)
#install.packages("lattice")
library(lattice)

# rounding to 2 decimal places
corr_m <- round(cor(tmp),2) 
head(corr_m)  
```
The credit limit and credit rating are the factors influencing each user's credit card balance in the current circumstance. These components may be utilized with linear regression machine learning to predict the average balance of each user.

The statement's correlation values show how strongly the variables are related to one another. In this instance, the correlations between the credit rating and balance (0.86) and the credit limit and balance (0.86) are both rather strong and near to 1. 

The variables have a significant positive association, as indicated by the correlation value of 0.86. This implies that the credit card balance tends to grow together with an increase in the credit limit or credit rating, and vice versa. When the correlation value approaches 1, the
```{r corrr, echo=FALSE}
#CORRELATION HEATMAP
#install.packages("reshape2")
library(reshape2)
# reduce the size of correlation matrix
melted_corr_mat <- melt(corr_m)
#head(melted_corr_mat)

# section c questio 2 plotting the correlation heatmap
ggplot(data = melted_corr_mat, aes(x=Var1, y=Var2,
                                   fill=value)) +
  geom_tile()
```


```{r  libraries2 }
# REGRESSION MODEL
library(dplyr)  # for data manipulation
library(ggplot2)  # for visualization
library(caret)  # for modeling
# Split the dataset into training and testing sets
set.seed(123)  # for reproducibility
```


```{r  trainTest }
train_index <- createDataPartition(data$Balance, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
```


```{r  linearMode }
# Train the model using linear regression
model <- train(Balance ~ Income + Rating + Cards + Age + Education + Gender , data = train_data, method = "lm")

```


```{r  important }
###FEATURE SELECTION
library(caret)
important_features <-varImp(model)
important_features

```
The most important features are shown above

```{r  besModel }
 
# Train the model using linear regression
model_best <- train(Balance ~ Income + Rating  + Age , data = train_data, method = "lm")

# Print the model summary
summary(model_best$finalModel)

```
Coefficients:

Intercept: -435.94; This is the predicted outcome when all independent variables are zero.
Income: -7.39; Each additional unit of income decreases the predicted outcome by 7.39. This is statistically significant (p < 0.001).
Rating: 3.85; Each additional unit of rating increases the predicted outcome by 3.85. This is also statistically significant (p < 0.001).
Age: -1.30; Each year of age decrease the predicted outcome by 1.30. This is statistically significant at the 0.05 level (p = 0.0226).


Interpretation

We may infer the following from the coefficients:

There is a favorable correlation between income and rating and the result. A better projected outcome is produced by higher income and credit rating, and this leads to a bigger credit balance.
Age has a detrimental impact on the result. Predicted credit balances are often smaller for older folks.
```{r  rmse_predc }
# Predict using the test set
predictions <- predict(model_best, newdata = test_data)

# Calculate evaluation metrics
RMSE <- sqrt(mean((test_data$Balance - predictions)^2))
R2 <- cor(predictions, test_data$Balance)^2

# Print evaluation metrics
print(paste0("RMSE: ", RMSE))
print(paste0("R-squared: ", R2))

```
the RMSE value is 171.970 and the R-squared value is 0.868. These values suggest that the model performs fairly well in predicting the credit balance using the given independent variables


## conclusion

The analysis's purview is restricted to the  few independent variables—as well as the particular dataset that was used to train the model. As a result, the results' applicability to other datasets that share the same independent variables may be restricted.

The analysis's possible drawback stems from the presumption that the connection between the independent variables and the outcome is linear. The model might not accurately represent the underlying connection between the variables if there are non-linear correlations or interactions between them. Furthermore, there might be bias from missing variables if the variables in the study did not fully represent all the factors impacting credit balance.

More independent factors or interactions might be taken into consideration to strengthen the study and provide a more compelling explanation for the credit balance change.


Although the model works well, it can always be made better. Here are a few options:

Add more features: At the moment, the model only takes three independent variables into account. The forecast accuracy may be increased by including more pertinent data, such as job status, debt-to-income ratio, or credit use rate.
Examine non-linear relationships: The independent and dependent variables are assumed to have a linear connection in the present model. Using methods such as polynomial or spline regression to explore non-linear connections may help capture more intricate interactions and increase accuracy.
Enhance the hyperparameters: Improved performance could result from fine-tuning the regularization parameters and other hyperparameters of the linear regression model.

 