---
title: "Statistical Learning"
output:
  pdf_document: default
  html_document: default
date: "2024-02-28"
---




__8__
__(a)__

```{r}
library("ISLR")

lm.fit <- lm(mpg ~ horsepower, data = Auto)
summary(lm.fit)
```

__i. Is there a relationship between the predictor and the response?__

__The p-values for the regression coefficients are nearly zero. This implies statistical significance, which in turn mean that there is a relationship.__

__ii. How strong is the relationship between the predictor and the response?__

__The R^{2} value indicates that about 61% of the variation in the response variable ( mpg) is due to the predictor variable (horsepower).__

__iii. Is the relationship between the predictor and the response positive or negative?__

__The regression coefficient for ‘horsepower’ is negative. Hence, the relationship is negative.__

__iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence and prediction intervals?__

```{r}
predict(lm.fit, data.frame(horsepower = c(98)), interval ="confidence")
```

```{r}
predict(lm.fit, data.frame(horsepower = c(98)), interval ="prediction")
```

__b__

```{r}
attach(Auto)
plot(mpg~horsepower, main =" MPG vs Horsepower", xlab = " Horsepower", ylab ="MPG")
abline(coef = coef(lm.fit), col ="red")
```

__c__

```{r}
plot(lm.fit)
```



__9__
__(c)Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output.__

```{r}
auto.mlr = lm(mpg ~ . -name, data=Auto)
summary(auto.mlr)
```

__i. Is there a relationship between the predictors and the response?__

__There are multiple predictors that have a relationship with the response because their associated p-value is significant. The p-value tells us the probability that the coefficient will take a value of 0. The typical threshold for p-value is 0.05. If the probability is below 0.05, then that means chances that it will be 0 is very slim.__

__ii. Which predictors appear to have a statistically significant relationship to the response?__

__The predictors: displacement, weight, year, and origin have a statistically significant relationship.__

__iii. What does the coefficient for the year variable suggest?__

__The coefficient of year is 0.7507 which is about 3/4. This tells us the relationship between year and MPG. It suggests that every 3 years, the mpg goes up by 4.__




__10(a) Fit MLR on the “Carseats” data set to predict “Sales” Using “Price”, “Urban”, “US”__

```{r}
library(ISLR)
data("Carseats")
dim(Carseats)
```


```{r}
lm.fit = lm(Sales ~ Price + Urban + US,data=Carseats)
summary(lm.fit)
```


__(b) Provide an interpretation of each coefficient in the model.__

```{r}
attach(Carseats)
str(data.frame(Price, Urban, US))
```


__“Price” variable: The average effect of a price increase of 1 dollar is a decrease of 54.4588492 units in sales all other predictors remaining fixed.__
__“Urban” variable: On average the unit sales in urban location are 21.9161508 units less than in rural location all other predictors remaining fixed.__
__“US” variable: On average the unit sales in a US store are 1200.5726978 units more than in a non US store all other predictors remaining fixed.__

__(c) Write out the model in equation form, being careful to handle the qualitative variables properly.__

__The model may be written as__
__Sales=13.0434689+(−0.0544588)×Price+(−0.0219162)×Urban+(1.2005727)×US+ε__
__with Urban=1if the store is in an urban location and 0,if not, and US=1,if the store is in the US and 0,if not.__

__(d)For which of the predictors can you reject the null hypothesis H0:βj=0?__

__We can reject the null hypothesis for the “Price” and “US” variables.__

__(e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.__

```{r}
lm.fit2 = lm(Sales ~ Price + US)
summary(lm.fit2)
```

__15(a)__

```{r}
library(MASS) 
library(tidyverse)
```

```{r}
lm.function = function(predictor) {
  fit1 <- lm(crim ~ predictor, data = Boston)
  return(summary(fit1))
}
```

```{r}
lm.zn = lm(crim ~ zn, data = Boston)
summary(lm.zn)
```

```{r}
par(mfrow = c(2,2))
plot(lm.zn)
```

```{r}
library(car)
```

```{r}
lm.indus = lm(crim ~ indus, data = Boston)
summary(lm.indus)
```

```{r}
par(mfrow = c(2,2))
plot(lm.indus)
```

```{r}
lm.chas = lm(crim ~ chas, data = Boston)
summary(lm.chas)
```

```{r}
lm.nox = lm(crim ~ nox, data = Boston)
summary(lm.nox)
```

```{r}
lm.rm = lm(crim ~ rm, data = Boston)
summary(lm.rm)
```

```{r}
lm.age = lm(crim ~ age, data = Boston)
summary(lm.age)
```

```{r}
lm.dis = lm(crim ~ dis, data = Boston)
summary(lm.dis)
```

```{r}
lm.rad = lm(crim ~ rad, data = Boston)
summary(lm.rad)
```

```{r}
lm.tax = lm(crim ~ tax, data = Boston)
summary(lm.tax)
```

```{r}
lm.ptratio = lm(crim ~ ptratio, data = Boston)
summary(lm.ptratio)
```

```{r}
attach(Boston)
lm.black = lm(crim~black)
summary(lm.black)
```

```{r}
lm.lstat = lm(crim ~ lstat, data = Boston)
summary(lm.lstat)
```

```{r}
lm.medv = lm(crim ~ medv, data = Boston)
summary(lm.medv)
```

```{r}
par(mfrow = c(2,2))
plot(lm.indus)
```

```{r}
plot(lm.chas)
```

```{r}
plot(lm.nox)

```

```{r}
plot(lm.rm)
```

```{r}
plot(lm.age)

```

```{r}
plot(lm.dis)

```

```{r}
plot(lm.rad)

```

```{r}
plot(lm.tax)

```

```{r}
plot(lm.ptratio)

```

```{r}
plot(lm.lstat)

```

```{r}
plot(lm.medv)
```

__(b)__

```{r}
lm.all = lm(crim ~ ., data=Boston)
summary(lm.all)
```

__(c)__

```{r}
x = c(coefficients(lm.zn)[2],
      coefficients(lm.indus)[2],
      coefficients(lm.chas)[2],
      coefficients(lm.nox)[2],
      coefficients(lm.rm)[2],
      coefficients(lm.age)[2],
      coefficients(lm.dis)[2],
      coefficients(lm.rad)[2],
      coefficients(lm.tax)[2],
      coefficients(lm.ptratio)[2],
      coefficients(lm.black)[2],
      coefficients(lm.lstat)[2],
      coefficients(lm.medv)[2])
y = coefficients(lm.all)[2:14]
```

```{r}
x
```

```{r}
y
```

```{r}
par(mfrow = c(1,1)) # 1 plot
plot(x, y)
```

__(d)__

```{r}
lm.poly.zn = lm(crim ~ zn + I(zn^2) + I(zn^3), data = Boston)
summary(lm.poly.zn)
```

```{r}
par(mfrow = c(2,2))
plot(lm.zn)
```

```{r}
lm.poly.indus = lm(crim ~ indus + I(indus^2) + I(indus^3), data = Boston)
summary(lm.poly.indus)
```

```{r}
lm.poly.chas = lm(crim ~ chas + I(chas^2) + I(chas^3), data = Boston)
summary(lm.poly.chas)
```

```{r}
lm.poly.nox = lm(crim ~ nox + I(nox^2) + I(nox^3), data = Boston)
summary(lm.poly.nox)
```

```{r}
lm.poly.rm = lm(crim ~ rm + I(rm^2) + I(rm^3), data = Boston)
summary(lm.poly.rm)
```

```{r}
lm.poly.age = lm(crim ~ age + I(age^2) + I(age^3), data = Boston)
summary(lm.poly.age)
```

```{r}
lm.poly.dis = lm(crim ~ dis + I(dis^2) + I(dis^3), data = Boston)
summary(lm.poly.dis)
```

```{r}
lm.poly.rad = lm(crim ~ rad + I(rad^2) + I(rad^3), data = Boston)
summary(lm.poly.rad)
```

```{r}
lm.poly.tax = lm(crim ~ tax + I(tax^2) + I(tax^3), data = Boston)
summary(lm.poly.tax)
```

```{r}
lm.poly.ptratio = lm(crim ~ ptratio + I(ptratio^2) + I(ptratio^3), data = Boston)
summary(lm.poly.ptratio)
```

```{r}
lm.poly.lstat = lm(crim ~ lstat + I(lstat^2) + I(lstat^3), data = Boston)
summary(lm.poly.lstat)
```

```{r}
lm.poly.medv = lm(crim ~ medv + I(medv^2) + I(medv^3), data = Boston)
summary(lm.poly.medv)
```


